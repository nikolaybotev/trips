# Starburst CSV Import Setup Guide

## Overview
This guide helps you import the trip CSV files generated by this project into Starburst using Method 1 (Hive Connector with External Location).

## Prerequisites
- Starburst Galaxy or Starburst Enterprise access
- AWS S3 bucket (or compatible object storage)
- AWS CLI configured (or alternative upload method)

## Step 1: Upload CSV Files to S3

### Option A: Using AWS CLI
```bash
# Create S3 bucket (if it doesn't exist)
aws s3 mb s3://your-trips-bucket

# Upload all trip CSV files
aws s3 cp tmp/ s3://your-trips-bucket/trips-data/ --recursive

# Verify upload
aws s3 ls s3://your-trips-bucket/trips-data/
```

### Option B: Using AWS Console
1. Go to AWS S3 Console
2. Create a new bucket or select existing bucket
3. Create folder: `trips-data/`
4. Upload all CSV files from `tmp/` directory

## Step 2: Configure Starburst Connector

### For Starburst Galaxy:
1. Go to Catalogs â†’ Add Catalog
2. Select "Hive" connector
3. Configure with your S3 bucket details

### For Starburst Enterprise:
1. Configure Hive connector in `etc/catalog/hive.properties`:
```properties
connector.name=hive
hive.metastore.uri=thrift://your-metastore:9083
hive.s3.aws-access-key=your-access-key
hive.s3.aws-secret-key=your-secret-key
hive.s3.endpoint=https://s3.amazonaws.com
```

## Step 3: Execute SQL Script

Run the SQL commands from `starburst_import.sql`:

1. **Create Schema, Table, and View**:
```sql
CREATE SCHEMA IF NOT EXISTS trips_data;

-- Raw table with VARCHAR columns (required for CSV format)
CREATE TABLE trips_data.trips_raw (
    user_id VARCHAR,
    trip_start_time VARCHAR,
    trip_end_time VARCHAR,
    start_lat VARCHAR,
    start_lng VARCHAR,
    end_lat VARCHAR,
    end_lng VARCHAR
)
WITH (
    format = 'CSV',
    external_location = 's3://your-trips-bucket/trips-data/',
    skip_header_line_count = 1
);

-- View with proper data types (lat/lng as DOUBLE)
-- Note: Using REPLACE to handle ISO timestamp format
CREATE VIEW trips_data.trips AS
SELECT
    user_id,
    CAST(REPLACE(REPLACE(trip_start_time, 'T', ' '), 'Z', '') AS TIMESTAMP) AS trip_start_time,
    CAST(REPLACE(REPLACE(trip_end_time, 'T', ' '), 'Z', '') AS TIMESTAMP) AS trip_end_time,
    CAST(start_lat AS DOUBLE) AS start_lat,
    CAST(start_lng AS DOUBLE) AS start_lng,
    CAST(end_lat AS DOUBLE) AS end_lat,
    CAST(end_lng AS DOUBLE) AS end_lng
FROM trips_data.trips_raw;
```

2. **Test the Import**:
```sql
SELECT COUNT(*) FROM trips_data.trips;
```

## Step 4: Data Type Handling

The implementation uses a two-layer approach:

1. **Raw Table (`trips_raw`)**: Contains VARCHAR columns as required by CSV format
2. **View (`trips`)**: Automatically casts columns to proper types including DOUBLE for coordinates

**Benefits of this approach:**
- Coordinates (lat/lng) are automatically DOUBLE type in queries
- Timestamps are automatically TIMESTAMP type
- No manual casting needed in most queries
- Raw data remains accessible if needed

## Step 5: Common Queries

### Basic Analytics
```sql
-- Trip count per user
SELECT user_id, COUNT(*) as trip_count
FROM trips_data.trips
GROUP BY user_id
ORDER BY trip_count DESC;

-- Daily trip patterns
SELECT
    DATE(CAST(trip_start_time AS TIMESTAMP)) as trip_date,
    COUNT(*) as daily_trips
FROM trips_data.trips
GROUP BY DATE(CAST(trip_start_time AS TIMESTAMP))
ORDER BY trip_date;
```

### Geographic Analysis
```sql
-- Trips by region (lat/lng are already DOUBLE type)
SELECT
    CASE
        WHEN start_lat > 0 THEN 'Northern Hemisphere'
        ELSE 'Southern Hemisphere'
    END as hemisphere,
    COUNT(*) as trip_count
FROM trips_data.trips
GROUP BY hemisphere;

-- Distance calculation example (using lat/lng as DOUBLE)
SELECT
    user_id,
    start_lat,
    start_lng,
    end_lat,
    end_lng,
    -- Simple distance approximation (not precise, for demonstration)
    SQRT(POWER(end_lat - start_lat, 2) + POWER(end_lng - start_lng, 2)) as distance_approx
FROM trips_data.trips
LIMIT 10;
```

## Troubleshooting

### Common Issues:
1. **Permission Denied**: Ensure Starburst has S3 read permissions
2. **File Not Found**: Verify S3 path and file existence
3. **Schema Mismatch**: Ensure CSV headers match table schema
4. **Timestamp Casting Error**:
   - **Error**: `Value cannot be cast to timestamp: 2025-08-12T15:04:47.994870Z`
   - **Solution**: Use `REPLACE` functions to convert ISO format: `REPLACE(REPLACE(trip_start_time, 'T', ' '), 'Z', '')`
   - **Alternative**: Use `date_parse` function with format pattern
5. **Data Type Errors**: Use proper casting for non-VARCHAR operations

### Verification Commands:
```sql
-- Check table exists
SHOW TABLES FROM trips_data;

-- Check table structure
DESCRIBE trips_data.trips;

-- Sample data
SELECT * FROM trips_data.trips LIMIT 5;
```

## Performance Tips

1. **Partitioning**: Consider partitioning by date for large datasets
2. **Compression**: Use compressed CSV files (gzip) for better performance
3. **File Size**: Optimal file size is 100-200MB per file
4. **Parallel Processing**: Starburst will automatically parallelize across multiple files

## Next Steps

After successful import:
1. Create views with proper data types
2. Set up data quality checks
3. Create dashboards and reports
4. Schedule regular data updates
