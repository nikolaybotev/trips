# BigQuery Import Guide for Trip Data

This guide provides comprehensive options for importing CSV files generated by `generate_trips.py` into BigQuery.

## Quick Start

### Option 1: Shell Script (Recommended for most users)
```bash
# Set your project details
export PROJECT_ID="your-project-id"
export DATASET_ID="trips_data"
export TABLE_ID="trips"

# Import using direct method
./quick_import_to_bigquery.sh ./trips_data direct

# Import using GCS method (for large datasets)
export BUCKET_NAME="your-bucket-name"
./quick_import_to_bigquery.sh ./trips_data gcs
```

### Option 2: Python Script (For advanced users)
```bash
# Install dependencies
pip install google-cloud-bigquery google-cloud-storage pandas

# Run import
python3 import_csv_to_bigquery.py \
    --project-id "your-project-id" \
    --dataset-id "trips_data" \
    --table-id "trips" \
    --csv-folder "./trips_data" \
    --method 1
```

### Option 3: Manual bq Command
```bash
# Create dataset
bq mk --dataset --location=US your-project:trips_data

# Import single file
bq load --source_format=CSV \
    --skip_leading_rows=1 \
    your-project.trips_data.trips \
    trips.csv \
    user_id:STRING,trip_start_time:TIMESTAMP,trip_end_time:TIMESTAMP,start_lat:FLOAT,start_lng:FLOAT,end_lat:FLOAT,end_lng:FLOAT
```

## Files Overview

| File | Purpose | Best For |
|------|---------|----------|
| `CSV_TO_BIGQUERY_IMPORT_OPTIONS.md` | Comprehensive guide with all methods | Understanding all options |
| `import_csv_to_bigquery.py` | Python script with multiple import methods | Programmatic control |
| `quick_import_to_bigquery.sh` | Shell script for quick imports | Quick and easy imports |
| `setup_bigquery_tables.sql` | SQL to create BigQuery tables | Table setup |

## Method Comparison

| Method | Dataset Size | Pros | Cons |
|--------|--------------|------|------|
| **BigQuery Console** | < 1GB | Easy to use, visual interface | Manual process, file size limits |
| **bq Command Line** | 1-10GB | Fast, scriptable, good error handling | Requires command line knowledge |
| **GCS + BigQuery** | 10-100GB | Scalable, cost-effective, parallel processing | Two-step process, storage costs |
| **Python Script** | Any size | Full control, data transformation | Requires coding, memory usage |
| **Apache Beam** | > 100GB | Massive scale, distributed processing | Complex, overkill for small datasets |
| **Data Transfer Service** | Any size | Automated, scheduled | GCS required, limited flexibility |

## Data Schema

Your trip data has the following schema:
```sql
CREATE TABLE trips (
  user_id STRING NOT NULL,
  trip_start_time TIMESTAMP NOT NULL,
  trip_end_time TIMESTAMP NOT NULL,
  start_lat FLOAT64 NOT NULL,
  start_lng FLOAT64 NOT NULL,
  end_lat FLOAT64 NOT NULL,
  end_lng FLOAT64 NOT NULL
)
PARTITION BY DATE(trip_start_time)
CLUSTER BY user_id;
```

## Performance Tips

1. **Partitioning**: Use `PARTITION BY DATE(trip_start_time)` for better performance
2. **Clustering**: Use `CLUSTER BY user_id` for user-based queries
3. **Batch Size**: For streaming inserts, use 1000-10000 rows per batch
4. **File Size**: Keep individual CSV files under 1GB for best performance
5. **Parallel Processing**: Use GCS method for multiple large files

## Cost Considerations

- **BigQuery**: Pay for data processed (not stored)
- **GCS**: Pay for storage and network egress
- **Data Transfer Service**: Additional service costs
- **Apache Beam**: Pay for compute time

## Troubleshooting

### Common Issues

1. **Schema Mismatch**
   ```bash
   # Check your CSV format matches the expected schema
   head -5 trips.csv
   ```

2. **Permission Errors**
   ```bash
   # Ensure you have BigQuery permissions
   gcloud auth login
   gcloud config set project your-project-id
   ```

3. **File Size Limits**
   ```bash
   # Use GCS method for large files
   ./quick_import_to_bigquery.sh ./trips_data gcs
   ```

4. **Memory Issues**
   ```bash
   # Use streaming insert for large datasets
   python3 import_csv_to_bigquery.py --method 3
   ```

### Validation

After import, validate your data:
```sql
-- Check row count
SELECT COUNT(*) FROM `your-project.trips_data.trips`;

-- Check data quality
SELECT
  COUNT(*) as total_rows,
  COUNT(DISTINCT user_id) as unique_users,
  MIN(trip_start_time) as earliest_trip,
  MAX(trip_end_time) as latest_trip
FROM `your-project.trips_data.trips`;

-- Check for null values
SELECT
  COUNT(*) as total_rows,
  SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) as null_user_ids,
  SUM(CASE WHEN trip_start_time IS NULL THEN 1 ELSE 0 END) as null_start_times
FROM `your-project.trips_data.trips`;
```

## Next Steps

After importing your data:

1. **Create staypoints**: Use the Dataflow job to convert trips to staypoints
2. **Set up monitoring**: Monitor query performance and costs
3. **Optimize queries**: Use partitioning and clustering effectively
4. **Schedule updates**: Set up automated data refresh if needed

## Support

For issues or questions:
1. Check the comprehensive guide: `CSV_TO_BIGQUERY_IMPORT_OPTIONS.md`
2. Review error messages in the console
3. Check BigQuery quotas and limits
4. Verify GCP permissions and authentication
