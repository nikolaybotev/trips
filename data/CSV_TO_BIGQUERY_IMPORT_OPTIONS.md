# CSV to BigQuery Import Options

This guide covers all available methods to import CSV files generated by `generate_trips.py` into BigQuery, with detailed pros and cons for each approach.

## Overview

Your trip data generator creates CSV files with the following schema:
- `user_id`: UUID v4 identifier
- `trip_start_time`: ISO timestamp (UTC)
- `trip_end_time`: ISO timestamp (UTC)
- `start_lat`: Starting latitude (-90 to 90)
- `start_lng`: Starting longitude (-180 to 180)
- `end_lat`: Ending latitude (-90 to 90)
- `end_lng`: Ending longitude (-180 to 180)

## Option 1: BigQuery Console (Web UI)

### How to Use
1. Go to [BigQuery Console](https://console.cloud.google.com/bigquery)
2. Select your project
3. Click "Create Dataset" → "Create Table"
4. Choose "Upload" → "Browse" → Select CSV files
5. Configure schema and settings

### Pros
- **User-friendly**: No command line required
- **Visual interface**: Easy to see data preview
- **Schema detection**: Automatic schema inference
- **Error handling**: Visual error messages
- **Small files**: Good for testing and small datasets

### Cons
- **File size limit**: 100MB per file, 1GB total per upload
- **Manual process**: Not suitable for automation
- **No batching**: Can't process multiple files efficiently
- **Limited control**: Fewer configuration options

### Best For
- Small datasets (< 1GB total)
- One-time imports
- Testing and development
- Non-technical users

---

## Option 2: bq Command Line Tool

### How to Use
```bash
# Single file import
bq load --source_format=CSV \
  --skip_leading_rows=1 \
  --field_delimiter=, \
  --quote="" \
  project.dataset.trips \
  trips.csv \
  user_id:STRING,trip_start_time:TIMESTAMP,trip_end_time:TIMESTAMP,start_lat:FLOAT,start_lng:FLOAT,end_lat:FLOAT,end_lng:FLOAT

# Multiple files with wildcard
bq load --source_format=CSV \
  --skip_leading_rows=1 \
  project.dataset.trips \
  gs://your-bucket/trips/*.csv \
  user_id:STRING,trip_start_time:TIMESTAMP,trip_end_time:TIMESTAMP,start_lat:FLOAT,start_lng:FLOAT,end_lat:FLOAT,end_lng:FLOAT
```

### Pros
- **Automation friendly**: Can be scripted and automated
- **Batch processing**: Can handle multiple files
- **Flexible**: Many configuration options
- **Fast**: Direct API calls
- **Error handling**: Detailed error messages
- **Resumable**: Can retry failed imports

### Cons
- **Command line**: Requires technical knowledge
- **Schema definition**: Must define schema manually
- **File size limits**: Still subject to BigQuery limits
- **No streaming**: Batch-only processing

### Best For
- Medium datasets (1GB - 10GB)
- Automated workflows
- CI/CD pipelines
- Technical users

---

## Option 3: Google Cloud Storage + BigQuery

### How to Use
```bash
# 1. Upload CSV files to GCS
gsutil -m cp trips*.csv gs://your-bucket/trips/

# 2. Create external table (no data movement)
bq mk --external_table_definition=CSV=gs://your-bucket/trips/*.csv \
  project.dataset.trips_external

# 3. Or load from GCS to BigQuery table
bq load --source_format=CSV \
  --skip_leading_rows=1 \
  project.dataset.trips \
  gs://your-bucket/trips/*.csv \
  user_id:STRING,trip_start_time:TIMESTAMP,trip_end_time:TIMESTAMP,start_lat:FLOAT,start_lng:FLOAT,end_lat:FLOAT,end_lng:FLOAT
```

### Pros
- **Scalable**: No file size limits
- **Cost effective**: Pay only for storage and queries
- **Flexible**: Can use external tables or load data
- **Parallel processing**: Multiple files processed simultaneously
- **Resumable**: Can retry failed operations
- **Versioning**: GCS supports object versioning

### Cons
- **Two-step process**: Upload to GCS first
- **Storage costs**: Additional GCS storage costs
- **Network transfer**: Data moves over network
- **Complexity**: More moving parts

### Best For
- Large datasets (> 10GB)
- Production environments
- Cost-sensitive applications
- High availability requirements

---

## Option 4: Python Script with BigQuery Client Library

### How to Use
```python
from google.cloud import bigquery
from google.cloud import storage
import pandas as pd
import glob
import os

def import_csv_to_bigquery(csv_folder, project_id, dataset_id, table_id):
    client = bigquery.Client(project=project_id)

    # Define schema
    schema = [
        bigquery.SchemaField("user_id", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("trip_start_time", "TIMESTAMP", mode="REQUIRED"),
        bigquery.SchemaField("trip_end_time", "TIMESTAMP", mode="REQUIRED"),
        bigquery.SchemaField("start_lat", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("start_lng", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("end_lat", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("end_lng", "FLOAT", mode="REQUIRED"),
    ]

    # Get all CSV files
    csv_files = glob.glob(os.path.join(csv_folder, "*.csv"))

    for csv_file in csv_files:
        print(f"Processing {csv_file}")

        # Load data with pandas
        df = pd.read_csv(csv_file)

        # Convert timestamps
        df['trip_start_time'] = pd.to_datetime(df['trip_start_time'])
        df['trip_end_time'] = pd.to_datetime(df['trip_end_time'])

        # Upload to BigQuery
        job_config = bigquery.LoadJobConfig(
            schema=schema,
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
            source_format=bigquery.SourceFormat.CSV,
            skip_leading_rows=1
        )

        job = client.load_table_from_dataframe(
            df, f"{project_id}.{dataset_id}.{table_id}", job_config=job_config
        )
        job.result()  # Wait for job to complete

        print(f"Loaded {len(df)} rows from {csv_file}")

# Usage
import_csv_to_bigquery(
    csv_folder="./trips_data",
    project_id="your-project",
    dataset_id="trips_data",
    table_id="trips"
)
```

### Pros
- **Programmatic control**: Full control over the process
- **Data transformation**: Can modify data before import
- **Error handling**: Custom error handling and retry logic
- **Progress tracking**: Can show progress and status
- **Flexible**: Can handle complex data transformations
- **Integration**: Easy to integrate with existing Python workflows

### Cons
- **Memory usage**: Loads data into memory
- **Development time**: Requires coding
- **Error handling**: Must implement error handling
- **Performance**: Slower than direct API calls
- **Dependencies**: Requires additional Python libraries

### Best For
- Data transformation requirements
- Complex business logic
- Integration with existing Python workflows
- Custom error handling needs

---

## Option 5: Apache Beam Dataflow

### How to Use
```python
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.io.gcp.bigquery import WriteToBigQuery
from apache_beam.io import ReadFromText

def run_pipeline():
    options = PipelineOptions()

    with beam.Pipeline(options=options) as pipeline:
        # Read CSV files
        trips = (
            pipeline
            | 'ReadCSV' >> ReadFromText('gs://your-bucket/trips/*.csv')
            | 'ParseCSV' >> beam.Map(parse_csv_row)
            | 'ConvertTypes' >> beam.Map(convert_types)
        )

        # Write to BigQuery
        trips | 'WriteToBigQuery' >> WriteToBigQuery(
            table='project.dataset.trips',
            schema=bigquery.TableSchema(fields=[...]),
            write_disposition=WriteToBigQuery.WriteDisposition.WRITE_TRUNCATE
        )

def parse_csv_row(line):
    # Parse CSV row and return dictionary
    pass

def convert_types(row):
    # Convert string types to appropriate types
    pass
```

### Pros
- **Massive scale**: Can handle petabytes of data
- **Distributed processing**: Parallel processing across many workers
- **Fault tolerance**: Automatic retry and error handling
- **Streaming**: Can process data in real-time
- **Cost effective**: Pay only for processing time
- **Monitoring**: Built-in monitoring and logging

### Cons
- **Complexity**: Requires understanding of Apache Beam
- **Development time**: More complex to implement
- **Overkill**: Too complex for small datasets
- **Cost**: Can be expensive for small jobs
- **Learning curve**: Steep learning curve

### Best For
- Very large datasets (> 100GB)
- Real-time processing requirements
- Complex data transformations
- Production data pipelines

---

## Option 6: BigQuery Data Transfer Service

### How to Use
1. Go to BigQuery Console → Data Transfers
2. Create new transfer → Cloud Storage
3. Configure source (GCS bucket)
4. Configure destination (BigQuery table)
5. Schedule transfer (one-time or recurring)

### Pros
- **Automated**: Scheduled and automated transfers
- **Monitoring**: Built-in monitoring and alerts
- **Reliable**: Google-managed service
- **Incremental**: Can handle incremental updates
- **No coding**: Configuration-based setup

### Cons
- **GCS required**: Must use Google Cloud Storage
- **Limited flexibility**: Fewer customization options
- **Cost**: Additional service costs
- **Complexity**: More complex setup for simple use cases

### Best For
- Recurring data imports
- Automated data pipelines
- Non-technical users
- Production environments

---

## Option 7: Cloud Functions + BigQuery

### How to Use
```python
from google.cloud import bigquery
from google.cloud import storage
import functions_framework

@functions_framework.cloud_event
def import_csv_to_bigquery(cloud_event):
    # Triggered by GCS file upload
    file_name = cloud_event.data["name"]
    bucket_name = cloud_event.data["bucket"]

    # Import logic here
    client = bigquery.Client()
    # ... import code ...
```

### Pros
- **Event-driven**: Automatic triggering
- **Serverless**: No infrastructure management
- **Cost effective**: Pay per execution
- **Scalable**: Automatic scaling
- **Integration**: Easy GCS integration

### Cons
- **Execution time limit**: 9 minutes max
- **Memory limits**: Limited memory
- **Cold starts**: Initial latency
- **Complexity**: Requires cloud function setup

### Best For
- Event-driven imports
- Small to medium datasets
- Serverless architectures
- Real-time processing

---

## Recommendation Matrix

| Dataset Size | Use Case | Recommended Option |
|--------------|----------|-------------------|
| < 1GB | Testing/Development | BigQuery Console |
| 1-10GB | One-time import | bq Command Line |
| 10-100GB | Production | GCS + BigQuery |
| > 100GB | Large scale | Apache Beam Dataflow |
| Recurring | Automated | Data Transfer Service |
| Real-time | Event-driven | Cloud Functions |

## Quick Start Script

Here's a complete script that combines multiple approaches:

```bash
#!/bin/bash
# Complete CSV to BigQuery import script

PROJECT_ID="your-project"
DATASET_ID="trips_data"
TABLE_ID="trips"
BUCKET_NAME="your-bucket"
CSV_FOLDER="./trips_data"

# 1. Upload to GCS (for large datasets)
echo "Uploading CSV files to GCS..."
gsutil -m cp ${CSV_FOLDER}/*.csv gs://${BUCKET_NAME}/trips/

# 2. Create BigQuery dataset
echo "Creating BigQuery dataset..."
bq mk --dataset ${PROJECT_ID}:${DATASET_ID}

# 3. Import to BigQuery
echo "Importing to BigQuery..."
bq load --source_format=CSV \
  --skip_leading_rows=1 \
  --field_delimiter=, \
  --quote="" \
  ${PROJECT_ID}.${DATASET_ID}.${TABLE_ID} \
  gs://${BUCKET_NAME}/trips/*.csv \
  user_id:STRING,trip_start_time:TIMESTAMP,trip_end_time:TIMESTAMP,start_lat:FLOAT,start_lng:FLOAT,end_lat:FLOAT,end_lng:FLOAT

echo "Import completed!"
```

Choose the option that best fits your specific requirements, dataset size, and technical constraints.
