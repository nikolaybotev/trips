#!/usr/bin/env python3
"""
Comprehensive script to import CSV files generated by generate_trips.py into BigQuery.
Supports multiple import methods and configurations.
"""

import argparse
import glob
import os
import sys
from pathlib import Path
from typing import List, Optional
import pandas as pd
from google.cloud import bigquery
from google.cloud import storage
from google.cloud.exceptions import NotFound
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class BigQueryCSVImporter:
    """Class to handle CSV to BigQuery imports with multiple methods."""

    def __init__(self, project_id: str, dataset_id: str, table_id: str):
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.table_id = table_id
        self.client = bigquery.Client(project=project_id)
        self.storage_client = storage.Client(project=project_id)

        # Define schema for trip data
        self.schema = [
            bigquery.SchemaField("user_id", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("trip_start_time", "TIMESTAMP", mode="REQUIRED"),
            bigquery.SchemaField("trip_end_time", "TIMESTAMP", mode="REQUIRED"),
            bigquery.SchemaField("start_lat", "FLOAT", mode="REQUIRED"),
            bigquery.SchemaField("start_lng", "FLOAT", mode="REQUIRED"),
            bigquery.SchemaField("end_lat", "FLOAT", mode="REQUIRED"),
            bigquery.SchemaField("end_lng", "FLOAT", mode="REQUIRED"),
        ]

    def create_dataset_if_not_exists(self):
        """Create BigQuery dataset if it doesn't exist."""
        try:
            dataset_ref = self.client.dataset(self.dataset_id)
            self.client.get_dataset(dataset_ref)
            logger.info(f"Dataset {self.dataset_id} already exists")
        except NotFound:
            dataset = bigquery.Dataset(dataset_ref)
            dataset.location = "US"  # Set your preferred location
            dataset = self.client.create_dataset(dataset, timeout=30)
            logger.info(f"Created dataset {self.dataset_id}")

    def method_1_direct_pandas_import(self, csv_files: List[str],
                                    write_disposition: str = "WRITE_APPEND") -> bool:
        """
        Method 1: Direct import using pandas and BigQuery client library.
        Best for: Small to medium datasets, data transformation needs.
        """
        logger.info("Starting Method 1: Direct pandas import")

        try:
            self.create_dataset_if_not_exists()

            total_rows = 0
            for csv_file in csv_files:
                logger.info(f"Processing {csv_file}")

                # Read CSV with pandas
                df = pd.read_csv(csv_file)

                # Convert timestamps
                df['trip_start_time'] = pd.to_datetime(df['trip_start_time'])
                df['trip_end_time'] = pd.to_datetime(df['trip_end_time'])

                # Configure job
                job_config = bigquery.LoadJobConfig(
                    schema=self.schema,
                    write_disposition=getattr(bigquery.WriteDisposition, write_disposition),
                    source_format=bigquery.SourceFormat.CSV,
                    skip_leading_rows=1
                )

                # Load data
                table_ref = f"{self.project_id}.{self.dataset_id}.{self.table_id}"
                job = self.client.load_table_from_dataframe(
                    df, table_ref, job_config=job_config
                )
                job.result()  # Wait for completion

                total_rows += len(df)
                logger.info(f"Loaded {len(df)} rows from {csv_file}")

            logger.info(f"Method 1 completed. Total rows loaded: {total_rows}")
            return True

        except Exception as e:
            logger.error(f"Method 1 failed: {e}")
            return False

    def method_2_gcs_import(self, csv_files: List[str], bucket_name: str,
                          write_disposition: str = "WRITE_APPEND") -> bool:
        """
        Method 2: Upload to GCS first, then import to BigQuery.
        Best for: Large datasets, production environments.
        """
        logger.info("Starting Method 2: GCS import")

        try:
            # Create GCS bucket if it doesn't exist
            try:
                bucket = self.storage_client.bucket(bucket_name)
                bucket.reload()
                logger.info(f"Bucket {bucket_name} already exists")
            except NotFound:
                bucket = self.storage_client.create_bucket(bucket_name)
                logger.info(f"Created bucket {bucket_name}")

            # Upload CSV files to GCS
            gcs_paths = []
            for csv_file in csv_files:
                blob_name = f"trips/{os.path.basename(csv_file)}"
                blob = bucket.blob(blob_name)
                blob.upload_from_filename(csv_file)
                gcs_paths.append(f"gs://{bucket_name}/{blob_name}")
                logger.info(f"Uploaded {csv_file} to gs://{bucket_name}/{blob_name}")

            # Create dataset if needed
            self.create_dataset_if_not_exists()

            # Configure job for GCS import
            job_config = bigquery.LoadJobConfig(
                schema=self.schema,
                write_disposition=getattr(bigquery.WriteDisposition, write_disposition),
                source_format=bigquery.SourceFormat.CSV,
                skip_leading_rows=1,
                field_delimiter=",",
                quote_character='"'
            )

            # Load from GCS
            table_ref = f"{self.project_id}.{self.dataset_id}.{self.table_id}"
            job = self.client.load_table_from_uri(
                gcs_paths, table_ref, job_config=job_config
            )
            job.result()  # Wait for completion

            logger.info(f"Method 2 completed. Loaded from {len(gcs_paths)} files")
            return True

        except Exception as e:
            logger.error(f"Method 2 failed: {e}")
            return False

    def method_3_streaming_insert(self, csv_files: List[str],
                                batch_size: int = 1000) -> bool:
        """
        Method 3: Streaming insert for real-time data.
        Best for: Real-time data, small batches.
        """
        logger.info("Starting Method 3: Streaming insert")

        try:
            self.create_dataset_if_not_exists()

            # Create table if it doesn't exist
            table_ref = f"{self.project_id}.{self.dataset_id}.{self.table_id}"
            try:
                table = self.client.get_table(table_ref)
            except NotFound:
                table = bigquery.Table(table_ref, schema=self.schema)
                table = self.client.create_table(table)
                logger.info(f"Created table {table_ref}")

            total_rows = 0
            for csv_file in csv_files:
                logger.info(f"Processing {csv_file}")

                # Read and process CSV
                df = pd.read_csv(csv_file)
                df['trip_start_time'] = pd.to_datetime(df['trip_start_time'])
                df['trip_end_time'] = pd.to_datetime(df['trip_end_time'])

                # Convert to list of dictionaries
                rows_to_insert = df.to_dict('records')

                # Insert in batches
                for i in range(0, len(rows_to_insert), batch_size):
                    batch = rows_to_insert[i:i + batch_size]
                    errors = self.client.insert_rows_json(table, batch)

                    if errors:
                        logger.error(f"Insert errors: {errors}")
                        return False

                    total_rows += len(batch)
                    logger.info(f"Inserted batch of {len(batch)} rows")

            logger.info(f"Method 3 completed. Total rows inserted: {total_rows}")
            return True

        except Exception as e:
            logger.error(f"Method 3 failed: {e}")
            return False

    def get_table_info(self) -> dict:
        """Get information about the target table."""
        try:
            table_ref = f"{self.project_id}.{self.dataset_id}.{self.table_id}"
            table = self.client.get_table(table_ref)

            return {
                "table_id": table.table_id,
                "num_rows": table.num_rows,
                "num_bytes": table.num_bytes,
                "created": table.created,
                "modified": table.modified,
                "schema": [field.to_api_repr() for field in table.schema]
            }
        except NotFound:
            return {"error": "Table not found"}
        except Exception as e:
            return {"error": str(e)}


def main():
    parser = argparse.ArgumentParser(description="Import CSV files to BigQuery")
    parser.add_argument("--project-id", required=True, help="GCP Project ID")
    parser.add_argument("--dataset-id", required=True, help="BigQuery Dataset ID")
    parser.add_argument("--table-id", required=True, help="BigQuery Table ID")
    parser.add_argument("--csv-folder", required=True, help="Folder containing CSV files")
    parser.add_argument("--method", choices=["1", "2", "3"], default="1",
                       help="Import method (1=pandas, 2=gcs, 3=streaming)")
    parser.add_argument("--bucket-name", help="GCS bucket name (required for method 2)")
    parser.add_argument("--write-disposition", choices=["WRITE_APPEND", "WRITE_TRUNCATE"],
                       default="WRITE_APPEND", help="Write disposition")
    parser.add_argument("--batch-size", type=int, default=1000,
                       help="Batch size for streaming insert (method 3)")
    parser.add_argument("--info", action="store_true", help="Show table information")

    args = parser.parse_args()

    # Validate arguments
    if args.method == "2" and not args.bucket_name:
        logger.error("Bucket name is required for method 2")
        sys.exit(1)

    # Get CSV files
    csv_pattern = os.path.join(args.csv_folder, "*.csv")
    csv_files = glob.glob(csv_pattern)

    if not csv_files:
        logger.error(f"No CSV files found in {args.csv_folder}")
        sys.exit(1)

    logger.info(f"Found {len(csv_files)} CSV files: {csv_files}")

    # Create importer
    importer = BigQueryCSVImporter(args.project_id, args.dataset_id, args.table_id)

    # Show table info if requested
    if args.info:
        info = importer.get_table_info()
        print("\nTable Information:")
        for key, value in info.items():
            print(f"  {key}: {value}")
        return

    # Run import based on method
    success = False
    if args.method == "1":
        success = importer.method_1_direct_pandas_import(csv_files, args.write_disposition)
    elif args.method == "2":
        success = importer.method_2_gcs_import(csv_files, args.bucket_name, args.write_disposition)
    elif args.method == "3":
        success = importer.method_3_streaming_insert(csv_files, args.batch_size)

    if success:
        logger.info("Import completed successfully!")

        # Show final table info
        info = importer.get_table_info()
        if "error" not in info:
            print(f"\nFinal table info:")
            print(f"  Rows: {info['num_rows']:,}")
            print(f"  Size: {info['num_bytes'] / (1024**3):.2f} GB")
    else:
        logger.error("Import failed!")
        sys.exit(1)


if __name__ == "__main__":
    main()
